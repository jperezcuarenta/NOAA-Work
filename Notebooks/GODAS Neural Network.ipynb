{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9ca87bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netCDF4 in /home/jperez/miniconda3/lib/python3.8/site-packages (1.5.6)\r\n",
      "Requirement already satisfied: cftime in /home/jperez/miniconda3/lib/python3.8/site-packages (from netCDF4) (1.5.0)\r\n",
      "Requirement already satisfied: numpy>=1.9 in /home/jperez/miniconda3/lib/python3.8/site-packages (from netCDF4) (1.20.3)\r\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "!pip install netCDF4\n",
    "\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "782e4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaffold code to load in data.  This code cell is mostly data wrangling\n",
    "def plot_nino_time_series(y, predictions, depth, month_lead, title):\n",
    "  \"\"\"\n",
    "  inputs\n",
    "  ------\n",
    "    y           pd.Series : time series of the true Nino index\n",
    "    predictions np.array  : time series of the predicted Nino index (same\n",
    "                            length and time as y)\n",
    "    titile                : the title of the plot\n",
    "\n",
    "  outputs\n",
    "  -------\n",
    "    None.  Displays the plot\n",
    "  \"\"\"\n",
    "  predictions = pd.Series(predictions, index=y.index)\n",
    "  predictions = predictions.sort_index()\n",
    "  y = y.sort_index()\n",
    "\n",
    "  images_dir = '/cw3e/mead/projects/cdd101/Figures/NOAA_GODAS_Results_NeuralNetwork'\n",
    "  figName_png = str(depth)+'m_Predictions_'+str(month_lead)+'month.png'\n",
    "  figName_pdf = str(depth)+'m_Predictions_'+str(month_lead)+'month.pdf'\n",
    "\n",
    "  plt.plot(y, label='Ground Truth')\n",
    "  plt.plot(predictions, '--', label='ML Predictions')\n",
    "  plt.legend(loc='best')\n",
    "  plt.title(title)\n",
    "  plt.ylabel('Nino3.4 Index')\n",
    "  plt.xlabel('Date')\n",
    "#  plt.show()\n",
    "\n",
    "  plt.savefig(images_dir+\"/\"+figName_png, bbox_inches='tight')\n",
    "  plt.savefig(images_dir+\"/\"+figName_pdf, bbox_inches='tight')\n",
    "\n",
    "  plt.close()\n",
    "\n",
    "def load_enso_indices():\n",
    "  \"\"\"\n",
    "  Reads in the txt data file to output a pandas Series of ENSO vals\n",
    "\n",
    "  outputs\n",
    "  -------\n",
    "\n",
    "    pd.Series : monthly ENSO values starting from 1980-01-01\n",
    "  \"\"\"\n",
    "  with open(anomaly_path) as f:\n",
    "    line = f.readline()\n",
    "    enso_vals = []\n",
    "    while line:\n",
    "        yearly_enso_vals = map(float, line.split()[1:])\n",
    "        enso_vals.extend(yearly_enso_vals)\n",
    "        line = f.readline()\n",
    "\n",
    "  enso_vals = pd.Series(enso_vals)\n",
    "  enso_vals.index = pd.date_range('1980-01-01',freq='MS',\n",
    "                                  periods=len(enso_vals))\n",
    "  enso_vals.index = pd.to_datetime(enso_vals.index)\n",
    "  return enso_vals\n",
    "\n",
    "def assemble_predictors_predictands(start_date, end_date, lead_time, \n",
    "                                    dataset, data_format,\n",
    "                                    num_input_time_steps=1,\n",
    "                                    use_pca=False, n_components=32,\n",
    "                                    lat_slice=None, lon_slice=None):\n",
    "  \"\"\"\n",
    "  inputs\n",
    "  ------\n",
    "\n",
    "      start_date           str : the start date from which to extract sst\n",
    "      end_date             str : the end date \n",
    "      lead_time            str : the number of months between each sst\n",
    "                              value and the target Nino3.4 Index\n",
    "      dataset              str : 'observations' 'CNRM' or 'MPI'\n",
    "      data_format          str : 'spatial' or 'flatten'. 'spatial' preserves\n",
    "                                  the lat/lon dimensions and returns an \n",
    "                                  array of shape (num_samples, num_input_time_steps,\n",
    "                                  lat, lon).  'flatten' returns an array of shape\n",
    "                                  (num_samples, num_input_time_steps*lat*lon)\n",
    "      num_input_time_steps int : the number of time steps to use for each \n",
    "                                 predictor sample\n",
    "      use_pca             bool : whether or not to apply principal components\n",
    "                              analysis to the sst field\n",
    "      n_components         int : the number of components to use for PCA\n",
    "      lat_slice           slice: the slice of latitudes to use \n",
    "      lon_slice           slice: the slice of longitudes to use\n",
    "\n",
    "  outputs\n",
    "  -------\n",
    "      Returns a tuple of the predictors (np array of sst temperature anomalies) \n",
    "      and the predictands (np array the ENSO index at the specified lead time).\n",
    "\n",
    "  \"\"\"\n",
    "  file_name = {'observations' : godas_path,\n",
    "               'CNRM'         : 'CNRM_tas_anomalies_regridded.nc',\n",
    "               'MPI'          : 'MPI_tas_anomalies_regridded.nc'}[dataset]\n",
    "  variable_name = {'observations' : 'deepTemp',\n",
    "                   'CNRM'         : 'tas',\n",
    "                   'MPI'          : 'tas'}[dataset]\n",
    "  ds = xr.open_dataset(file_name)\n",
    "  sst = ds[variable_name].sel(time=slice(start_date, end_date))\n",
    "  if lat_slice is not None:\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    raise NotImplementedError(\"In EXERCISE 7, you must implement the slicing!\")\n",
    "  if lon_slice is not None:\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    raise NotImplementedError(\"In EXERCISE 7, you must implement the slicing!\")\n",
    "  \n",
    "  \n",
    "  num_samples = sst.shape[0]\n",
    "  #sst is a (num_samples, lat, lon) array\n",
    "  #the line below converts it to (num_samples, num_input_time_steps, lat, lon)\n",
    "  sst = np.stack([sst.values[n-num_input_time_steps:n] for n in range(num_input_time_steps,\n",
    "                                                              num_samples+1)])\n",
    "  #CHALLENGE: CAN YOU IMPLEMENT THE ABOVE LINE WITHOUT A FOR LOOP?\n",
    "  num_samples = sst.shape[0]\n",
    "\n",
    "  sst[np.isnan(sst)] = 0\n",
    "  if data_format=='flatten':\n",
    "    #sst is a 3D array: (time_steps, lat, lon)\n",
    "    #in this tutorial, we will not be using ML models that take\n",
    "    #advantage of the spatial nature of global temperature\n",
    "    #therefore, we reshape sst into a 2D array: (time_steps, lat*lon)\n",
    "    #(At each time step, there are lat*lon predictors)\n",
    "    sst = sst.reshape(num_samples, -1)\n",
    "    \n",
    "\n",
    "    #Use Principal Components Analysis, also called\n",
    "    #Empirical Orthogonal Functions, to reduce the\n",
    "    #dimensionality of the array\n",
    "    if use_pca:\n",
    "      pca = sklearn.decomposition.PCA(n_components=n_components)\n",
    "      pca.fit(sst)\n",
    "      X = pca.transform(sst)\n",
    "    else:\n",
    "      X = sst\n",
    "  else: # data_format=='spatial'\n",
    "    X = sst\n",
    "\n",
    "  start_date_plus_lead = pd.to_datetime(start_date) + \\\n",
    "                        pd.DateOffset(months=lead_time+num_input_time_steps-1)\n",
    "  end_date_plus_lead = pd.to_datetime(end_date) + \\\n",
    "                      pd.DateOffset(months=lead_time)\n",
    "  if dataset == 'observations':\n",
    "    y = load_enso_indices()[slice(start_date_plus_lead, \n",
    "                                  end_date_plus_lead)]\n",
    "  else: #the data is from a GCM\n",
    "    X = X.astype(np.float32)\n",
    "    #The Nino3.4 Index is composed of three month rolling values\n",
    "    #Therefore, when calculating the Nino3.4 Index in a GCM\n",
    "    #we have to extract the two months prior to the first target start date\n",
    "    target_start_date_with_2_month = start_date_plus_lead - pd.DateOffset(months=2)\n",
    "    subsetted_ds = ds[variable_name].sel(time=slice(target_start_date_with_2_month,\n",
    "                                                   end_date_plus_lead))\n",
    "    #Calculate the Nino3.4 index\n",
    "    y = subsetted_ds.sel(lat=slice(5,-5), lon=slice(360-170,360-120)).mean(dim=('lat','lon'))\n",
    "\n",
    "    y = pd.Series(y.values).rolling(window=3).mean()[2:].values\n",
    "    y = y.astype(np.float32)\n",
    "  ds.close()\n",
    "  return X.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "\n",
    "class ENSODataset(Dataset):\n",
    "    def __init__(self, predictors, predictands):\n",
    "        self.predictors = predictors\n",
    "        self.predictands = predictands\n",
    "        assert self.predictors.shape[0] == self.predictands.shape[0], \\\n",
    "               \"The number of predictors must equal the number of predictands!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.predictors.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.predictors[idx], self.predictands[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2f4fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_input_time_steps=1, print_feature_dimension=False):\n",
    "        \"\"\"\n",
    "        inputs\n",
    "        -------\n",
    "            num_input_time_steps        (int) : the number of input time\n",
    "                                                steps in the predictor\n",
    "            print_feature_dimension    (bool) : whether or not to print\n",
    "                                                out the dimension of the features\n",
    "                                                extracted from the conv layers\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_input_time_steps, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.print_layer = Print()\n",
    "        \n",
    "        #ATTENTION EXERCISE 9: print out the dimension of the extracted features from \n",
    "        #the conv layers for setting the dimension of the linear layer!\n",
    "        #Using the print_layer, we find that the dimensions are \n",
    "        #(batch_size, 16, 42, 87)\n",
    "#        self.fc1 = nn.Linear(16 * 42 * 87, 120)\n",
    "#        self.fc2 = nn.Linear(120, 84)\n",
    "#        self.fc3 = nn.Linear(84, 1)\n",
    "#        self.print_feature_dimension = print_feature_dimension\n",
    "        #10, 16, 102, 87\n",
    "        self.fc1 = nn.Linear(16 * 102 * 87, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "        self.print_feature_dimension = print_feature_dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        if self.print_feature_dimension:\n",
    "          x = self.print_layer(x)\n",
    "#        x = x.view(-1, 16 * 42 * 87)\n",
    "        x = x.view(-1, 16 * 102 * 87)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class Print(nn.Module):\n",
    "    \"\"\"\n",
    "    This class prints out the size of the features\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eabe28fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net, criterion, optimizer, trainloader, testloader, \n",
    "                  experiment_name, num_epochs=30):\n",
    "  \"\"\"\n",
    "  inputs\n",
    "  ------\n",
    "\n",
    "      net               (nn.Module)   : the neural network architecture\n",
    "      criterion         (nn)          : the loss function (i.e. root mean squared error)\n",
    "      optimizer         (torch.optim) : the optimizer to use update the neural network \n",
    "                                        architecture to minimize the loss function\n",
    "      trainloader       (torch.utils.data.DataLoader): dataloader that loads the\n",
    "                                        predictors and predictands\n",
    "                                        for the train dataset\n",
    "      testloader        (torch.utils.data. DataLoader): dataloader that loads the\n",
    "                                        predictors and predictands\n",
    "                                        for the test dataset\n",
    "  outputs\n",
    "  -------\n",
    "      predictions (np.array), and saves the trained neural network as a .pt file\n",
    "  \"\"\"\n",
    "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "  net = net.to(device)\n",
    "  best_loss = np.infty\n",
    "  train_losses, test_losses = [], []\n",
    "\n",
    "# 40 loops\n",
    "  for epoch in range(num_epochs):\n",
    "    # 31*13 loops\n",
    "    # torch size changes at final step in loop due to nn.RMSE for some reason\n",
    "    for mode, data_loader in [('train', trainloader), ('test', testloader)]:\n",
    "      #Set the model to train mode to allow its weights to be updated\n",
    "      #while training\n",
    "      if mode == 'train':\n",
    "        net.train()\n",
    "        print(\"Training\")\n",
    "\n",
    "      #Set the model to eval model to prevent its weights from being updated\n",
    "      #while testing\n",
    "      elif mode == 'test':\n",
    "        net.eval()\n",
    "        print(\"Testing\")\n",
    "\n",
    "      running_loss = 0.0\n",
    "      for i, data in enumerate(data_loader):\n",
    "          # get a mini-batch of predictors and predictands\n",
    "          batch_predictors, batch_predictands = data\n",
    "#          print(\"Predictant from Data, Predicant from .to:\")\n",
    "#          print(batch_predictands.size())\n",
    "          batch_predictands = batch_predictands.to(device)\n",
    "#          print(batch_predictands.size())\n",
    "          batch_predictors = batch_predictors.to(device)\n",
    "\n",
    "          # zero the parameter gradients\n",
    "          # I think this visits the layer \n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          #calculate the predictions of the current neural network\n",
    "#          print(\"Predictions by squeezing batch\")\n",
    "          predictions = net(batch_predictors).squeeze()\n",
    "#          print(predictions.size())\n",
    "\n",
    "          #quantify the quality of the predictions using a\n",
    "          #loss function (aka criterion) that is differentiable\n",
    "          # check if statement for predictions and batch_predictands \n",
    "          # having correct size \n",
    "#          print(batch_predictands.item())\n",
    "          if predictions.numel() == 1:\n",
    "#            print(\"I exist sometimes\")\n",
    "            # dim 0 tensor (scalar)\n",
    "#            print(predictions.size())\n",
    "            # dim 1 tensor\n",
    "#            print(batch_predictands.size())\n",
    "            batch_predictands = batch_predictands.squeeze(-1)\n",
    "#            print(batch_predictands.size())\n",
    "            loss = criterion(predictions, batch_predictands)\n",
    "          else:\n",
    "            loss = criterion(predictions, batch_predictands)\n",
    "\n",
    "          if mode == 'train':\n",
    "            #the 'backward pass: calculates the gradients of each weight\n",
    "            #of the neural network with respect to the loss)\n",
    "            loss.backward()\n",
    "\n",
    "            #the optimizer updates the weights of the neural network\n",
    "            #based on the gradients calculated above and the choice\n",
    "            #of optimization algorithm\n",
    "            optimizer.step()\n",
    "          \n",
    "          #Save the model weights that have the best performance!\n",
    "        \n",
    "\n",
    "          running_loss += loss.item()\n",
    "      if running_loss < best_loss and mode == 'test':\n",
    "          best_loss = running_loss\n",
    "          torch.save(net, '{}.pt'.format(experiment_name))\n",
    "\n",
    "      print('{} Set: Epoch {:02d}. loss: {:3f}'.format(mode, epoch+1, \\\n",
    "                                            running_loss/len(data_loader)))\n",
    "      if mode == 'train':\n",
    "          train_losses.append(running_loss/len(data_loader))\n",
    "      else:\n",
    "          test_losses.append(running_loss/len(data_loader))\n",
    "    \n",
    "  net = torch.load('{}.pt'.format(experiment_name))\n",
    "  net.eval()\n",
    "  net.to(device)\n",
    "  \n",
    "  #the remainder of this notebook calculates the predictions of the best\n",
    "  #saved model\n",
    "  predictions = np.asarray([])\n",
    "  for i, data in enumerate(testloader):\n",
    "    batch_predictors, batch_predictands = data\n",
    "    batch_predictands = batch_predictands.to(device)\n",
    "    batch_predictors = batch_predictors.to(device)\n",
    "\n",
    "    batch_predictions = net(batch_predictors).squeeze()\n",
    "    #Edge case: if there is 1 item in the batch, batch_predictions becomes a float\n",
    "    #not a Tensor. the if statement below converts it to a Tensor\n",
    "    #so that it is compatible with np.concatenate\n",
    "    if len(batch_predictions.size()) == 0:\n",
    "      batch_predictions = torch.Tensor([batch_predictions])\n",
    "    predictions = np.concatenate([predictions, batch_predictions.detach().cpu().numpy()])\n",
    "  return predictions, train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "729f6972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "train Set: Epoch 01. loss: 3.571097\n",
      "Testing\n",
      "test Set: Epoch 01. loss: 5.074559\n",
      "Training\n",
      "train Set: Epoch 02. loss: 1.937912\n",
      "Testing\n",
      "test Set: Epoch 02. loss: 4.814180\n",
      "Training\n",
      "train Set: Epoch 03. loss: 2.124040\n",
      "Testing\n",
      "test Set: Epoch 03. loss: 3.384156\n",
      "Training\n",
      "train Set: Epoch 04. loss: 1.765587\n",
      "Testing\n",
      "test Set: Epoch 04. loss: 3.335305\n",
      "Training\n",
      "train Set: Epoch 05. loss: 1.635418\n",
      "Testing\n",
      "test Set: Epoch 05. loss: 3.601871\n",
      "Training\n",
      "train Set: Epoch 06. loss: 1.710638\n",
      "Testing\n",
      "test Set: Epoch 06. loss: 3.383120\n",
      "Training\n",
      "train Set: Epoch 07. loss: 1.654751\n",
      "Testing\n",
      "test Set: Epoch 07. loss: 3.372209\n",
      "Training\n",
      "train Set: Epoch 08. loss: 1.621786\n",
      "Testing\n",
      "test Set: Epoch 08. loss: 3.343249\n",
      "Training\n",
      "train Set: Epoch 09. loss: 1.590252\n",
      "Testing\n",
      "test Set: Epoch 09. loss: 3.321148\n",
      "Training\n",
      "train Set: Epoch 10. loss: 1.570402\n",
      "Testing\n",
      "test Set: Epoch 10. loss: 3.319604\n",
      "Training\n",
      "train Set: Epoch 11. loss: 1.531674\n",
      "Testing\n",
      "test Set: Epoch 11. loss: 3.309965\n",
      "Training\n",
      "train Set: Epoch 12. loss: 1.513190\n",
      "Testing\n",
      "test Set: Epoch 12. loss: 3.297781\n",
      "Training\n",
      "train Set: Epoch 13. loss: 1.492833\n",
      "Testing\n",
      "test Set: Epoch 13. loss: 3.284859\n",
      "Training\n",
      "train Set: Epoch 14. loss: 1.481422\n",
      "Testing\n",
      "test Set: Epoch 14. loss: 3.255958\n",
      "Training\n",
      "train Set: Epoch 15. loss: 1.440665\n",
      "Testing\n",
      "test Set: Epoch 15. loss: 3.258799\n",
      "Training\n",
      "train Set: Epoch 16. loss: 1.413731\n",
      "Testing\n",
      "test Set: Epoch 16. loss: 3.229664\n",
      "Training\n",
      "train Set: Epoch 17. loss: 1.382132\n",
      "Testing\n",
      "test Set: Epoch 17. loss: 3.272778\n",
      "Training\n",
      "train Set: Epoch 18. loss: 1.576938\n",
      "Testing\n",
      "test Set: Epoch 18. loss: 3.448427\n",
      "Training\n",
      "train Set: Epoch 19. loss: 1.388298\n",
      "Testing\n",
      "test Set: Epoch 19. loss: 3.012704\n",
      "Training\n",
      "train Set: Epoch 20. loss: 1.333260\n",
      "Testing\n",
      "test Set: Epoch 20. loss: 2.929388\n",
      "Training\n",
      "train Set: Epoch 21. loss: 1.126534\n",
      "Testing\n",
      "test Set: Epoch 21. loss: 3.229221\n",
      "Training\n",
      "train Set: Epoch 22. loss: 1.046250\n",
      "Testing\n",
      "test Set: Epoch 22. loss: 3.429446\n",
      "Training\n",
      "train Set: Epoch 23. loss: 0.994592\n",
      "Testing\n",
      "test Set: Epoch 23. loss: 3.229414\n",
      "Training\n",
      "train Set: Epoch 24. loss: 0.925812\n",
      "Testing\n",
      "test Set: Epoch 24. loss: 2.995720\n",
      "Training\n",
      "train Set: Epoch 25. loss: 0.884513\n",
      "Testing\n",
      "test Set: Epoch 25. loss: 2.107995\n",
      "Training\n",
      "train Set: Epoch 26. loss: 0.653816\n",
      "Testing\n",
      "test Set: Epoch 26. loss: 2.519730\n",
      "Training\n",
      "train Set: Epoch 27. loss: 0.530219\n",
      "Testing\n",
      "test Set: Epoch 27. loss: 2.465848\n",
      "Training\n",
      "train Set: Epoch 28. loss: 0.437093\n",
      "Testing\n",
      "test Set: Epoch 28. loss: 2.336391\n",
      "Training\n",
      "train Set: Epoch 29. loss: 0.372200\n",
      "Testing\n",
      "test Set: Epoch 29. loss: 2.319118\n",
      "Training\n",
      "train Set: Epoch 30. loss: 0.349423\n",
      "Testing\n",
      "test Set: Epoch 30. loss: 2.297053\n",
      "Training\n",
      "train Set: Epoch 01. loss: 4.416375\n",
      "Testing\n",
      "test Set: Epoch 01. loss: 4.656013\n",
      "Training\n",
      "train Set: Epoch 02. loss: 2.761735\n",
      "Testing\n",
      "test Set: Epoch 02. loss: 3.910522\n",
      "Training\n",
      "train Set: Epoch 03. loss: 2.601668\n",
      "Testing\n",
      "test Set: Epoch 03. loss: 3.590246\n",
      "Training\n",
      "train Set: Epoch 04. loss: 2.465107\n",
      "Testing\n",
      "test Set: Epoch 04. loss: 3.470030\n",
      "Training\n",
      "train Set: Epoch 05. loss: 2.375697\n",
      "Testing\n",
      "test Set: Epoch 05. loss: 3.500149\n",
      "Training\n",
      "train Set: Epoch 06. loss: 2.362265\n",
      "Testing\n",
      "test Set: Epoch 06. loss: 3.470535\n",
      "Training\n",
      "train Set: Epoch 07. loss: 2.336393\n",
      "Testing\n",
      "test Set: Epoch 07. loss: 3.466428\n",
      "Training\n",
      "train Set: Epoch 08. loss: 2.315530\n",
      "Testing\n",
      "test Set: Epoch 08. loss: 3.457109\n",
      "Training\n",
      "train Set: Epoch 09. loss: 2.296152\n",
      "Testing\n",
      "test Set: Epoch 09. loss: 3.448803\n",
      "Training\n",
      "train Set: Epoch 10. loss: 2.277545\n",
      "Testing\n",
      "test Set: Epoch 10. loss: 3.440851\n",
      "Training\n",
      "train Set: Epoch 11. loss: 2.260335\n",
      "Testing\n",
      "test Set: Epoch 11. loss: 3.432700\n",
      "Training\n",
      "train Set: Epoch 12. loss: 2.243892\n",
      "Testing\n",
      "test Set: Epoch 12. loss: 3.424525\n",
      "Training\n",
      "train Set: Epoch 13. loss: 2.228207\n",
      "Testing\n",
      "test Set: Epoch 13. loss: 3.416048\n",
      "Training\n",
      "train Set: Epoch 14. loss: 2.213023\n",
      "Testing\n",
      "test Set: Epoch 14. loss: 3.407226\n",
      "Training\n",
      "train Set: Epoch 15. loss: 2.198197\n",
      "Testing\n",
      "test Set: Epoch 15. loss: 3.397950\n",
      "Training\n",
      "train Set: Epoch 16. loss: 2.183577\n",
      "Testing\n",
      "test Set: Epoch 16. loss: 3.388133\n",
      "Training\n",
      "train Set: Epoch 17. loss: 2.168788\n",
      "Testing\n",
      "test Set: Epoch 17. loss: 3.377771\n",
      "Training\n",
      "train Set: Epoch 18. loss: 2.148561\n",
      "Testing\n",
      "test Set: Epoch 18. loss: 3.291445\n",
      "Training\n",
      "train Set: Epoch 19. loss: 2.678653\n",
      "Testing\n",
      "test Set: Epoch 19. loss: 3.511070\n",
      "Training\n",
      "train Set: Epoch 20. loss: 2.217899\n",
      "Testing\n",
      "test Set: Epoch 20. loss: 3.357643\n",
      "Training\n",
      "train Set: Epoch 21. loss: 2.100224\n",
      "Testing\n",
      "test Set: Epoch 21. loss: 3.371524\n",
      "Training\n",
      "train Set: Epoch 22. loss: 2.095267\n",
      "Testing\n",
      "test Set: Epoch 22. loss: 3.304025\n",
      "Training\n",
      "train Set: Epoch 23. loss: 2.062799\n",
      "Testing\n",
      "test Set: Epoch 23. loss: 3.299095\n",
      "Training\n",
      "train Set: Epoch 24. loss: 2.038060\n",
      "Testing\n",
      "test Set: Epoch 24. loss: 3.253944\n",
      "Training\n",
      "train Set: Epoch 25. loss: 2.003249\n",
      "Testing\n",
      "test Set: Epoch 25. loss: 3.235768\n",
      "Training\n",
      "train Set: Epoch 26. loss: 1.972619\n",
      "Testing\n",
      "test Set: Epoch 26. loss: 3.199432\n",
      "Training\n",
      "train Set: Epoch 27. loss: 1.933732\n",
      "Testing\n",
      "test Set: Epoch 27. loss: 3.166795\n",
      "Training\n",
      "train Set: Epoch 28. loss: 1.893112\n",
      "Testing\n",
      "test Set: Epoch 28. loss: 3.140519\n",
      "Training\n",
      "train Set: Epoch 29. loss: 1.857251\n",
      "Testing\n",
      "test Set: Epoch 29. loss: 3.091800\n",
      "Training\n",
      "train Set: Epoch 30. loss: 1.811146\n",
      "Testing\n",
      "test Set: Epoch 30. loss: 3.067505\n",
      "Training\n",
      "train Set: Epoch 01. loss: 0.677753\n",
      "Testing\n",
      "test Set: Epoch 01. loss: 0.379235\n",
      "Training\n",
      "train Set: Epoch 02. loss: 0.405116\n",
      "Testing\n",
      "test Set: Epoch 02. loss: 0.348300\n",
      "Training\n",
      "train Set: Epoch 03. loss: 0.423413\n",
      "Testing\n",
      "test Set: Epoch 03. loss: 0.277402\n",
      "Training\n",
      "train Set: Epoch 04. loss: 0.372277\n",
      "Testing\n",
      "test Set: Epoch 04. loss: 0.400184\n",
      "Training\n",
      "train Set: Epoch 05. loss: 0.349678\n",
      "Testing\n",
      "test Set: Epoch 05. loss: 0.455782\n",
      "Training\n",
      "train Set: Epoch 06. loss: 0.323664\n",
      "Testing\n",
      "test Set: Epoch 06. loss: 0.475117\n",
      "Training\n",
      "train Set: Epoch 07. loss: 0.341868\n",
      "Testing\n",
      "test Set: Epoch 07. loss: 0.417967\n",
      "Training\n",
      "train Set: Epoch 08. loss: 0.374812\n",
      "Testing\n",
      "test Set: Epoch 08. loss: 0.345492\n",
      "Training\n",
      "train Set: Epoch 09. loss: 0.313293\n",
      "Testing\n",
      "test Set: Epoch 09. loss: 0.279874\n",
      "Training\n",
      "train Set: Epoch 10. loss: 0.283284\n",
      "Testing\n",
      "test Set: Epoch 10. loss: 0.296069\n",
      "Training\n",
      "train Set: Epoch 11. loss: 0.275200\n",
      "Testing\n",
      "test Set: Epoch 11. loss: 0.291567\n",
      "Training\n",
      "train Set: Epoch 12. loss: 0.272877\n",
      "Testing\n",
      "test Set: Epoch 12. loss: 0.289939\n",
      "Training\n",
      "train Set: Epoch 13. loss: 0.267841\n",
      "Testing\n",
      "test Set: Epoch 13. loss: 0.292756\n",
      "Training\n",
      "train Set: Epoch 14. loss: 0.261944\n",
      "Testing\n",
      "test Set: Epoch 14. loss: 0.290103\n",
      "Training\n",
      "train Set: Epoch 15. loss: 0.256965\n",
      "Testing\n",
      "test Set: Epoch 15. loss: 0.288622\n",
      "Training\n",
      "train Set: Epoch 16. loss: 0.249954\n",
      "Testing\n",
      "test Set: Epoch 16. loss: 0.291248\n",
      "Training\n",
      "train Set: Epoch 17. loss: 0.244112\n",
      "Testing\n",
      "test Set: Epoch 17. loss: 0.285865\n",
      "Training\n",
      "train Set: Epoch 18. loss: 0.236646\n",
      "Testing\n",
      "test Set: Epoch 18. loss: 0.282849\n",
      "Training\n",
      "train Set: Epoch 19. loss: 0.229657\n",
      "Testing\n",
      "test Set: Epoch 19. loss: 0.282280\n",
      "Training\n",
      "train Set: Epoch 20. loss: 0.222168\n",
      "Testing\n",
      "test Set: Epoch 20. loss: 0.277621\n",
      "Training\n",
      "train Set: Epoch 21. loss: 0.213983\n",
      "Testing\n",
      "test Set: Epoch 21. loss: 0.275592\n",
      "Training\n",
      "train Set: Epoch 22. loss: 0.206044\n",
      "Testing\n",
      "test Set: Epoch 22. loss: 0.273470\n",
      "Training\n",
      "train Set: Epoch 23. loss: 0.198179\n",
      "Testing\n",
      "test Set: Epoch 23. loss: 0.272274\n",
      "Training\n",
      "train Set: Epoch 24. loss: 0.189144\n",
      "Testing\n",
      "test Set: Epoch 24. loss: 0.269552\n",
      "Training\n",
      "train Set: Epoch 25. loss: 0.180410\n",
      "Testing\n",
      "test Set: Epoch 25. loss: 0.269021\n",
      "Training\n",
      "train Set: Epoch 26. loss: 0.171320\n",
      "Testing\n",
      "test Set: Epoch 26. loss: 0.268927\n",
      "Training\n",
      "train Set: Epoch 27. loss: 0.161128\n",
      "Testing\n",
      "test Set: Epoch 27. loss: 0.267752\n",
      "Training\n",
      "train Set: Epoch 28. loss: 0.148944\n",
      "Testing\n",
      "test Set: Epoch 28. loss: 0.269159\n",
      "Training\n",
      "train Set: Epoch 29. loss: 0.145580\n",
      "Testing\n",
      "test Set: Epoch 29. loss: 0.216895\n",
      "Training\n",
      "train Set: Epoch 30. loss: 0.172762\n",
      "Testing\n",
      "test Set: Epoch 30. loss: 0.191315\n",
      "Training\n",
      "train Set: Epoch 01. loss: 4.842926\n",
      "Testing\n",
      "test Set: Epoch 01. loss: 2.945938\n",
      "Training\n",
      "train Set: Epoch 02. loss: 2.028134\n",
      "Testing\n",
      "test Set: Epoch 02. loss: 3.026967\n",
      "Training\n",
      "train Set: Epoch 03. loss: 1.923343\n",
      "Testing\n",
      "test Set: Epoch 03. loss: 2.981429\n",
      "Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Set: Epoch 04. loss: 1.849434\n",
      "Testing\n",
      "test Set: Epoch 04. loss: 2.967169\n",
      "Training\n",
      "train Set: Epoch 05. loss: 1.795049\n",
      "Testing\n",
      "test Set: Epoch 05. loss: 2.983115\n",
      "Training\n",
      "train Set: Epoch 06. loss: 1.798093\n",
      "Testing\n",
      "test Set: Epoch 06. loss: 3.471410\n",
      "Training\n",
      "train Set: Epoch 07. loss: 1.925127\n",
      "Testing\n",
      "test Set: Epoch 07. loss: 3.581055\n",
      "Training\n",
      "train Set: Epoch 08. loss: 1.801174\n",
      "Testing\n",
      "test Set: Epoch 08. loss: 3.700285\n",
      "Training\n",
      "train Set: Epoch 09. loss: 1.890083\n",
      "Testing\n",
      "test Set: Epoch 09. loss: 3.390498\n",
      "Training\n",
      "train Set: Epoch 10. loss: 1.840696\n",
      "Testing\n",
      "test Set: Epoch 10. loss: 4.225676\n",
      "Training\n",
      "train Set: Epoch 11. loss: 1.811436\n",
      "Testing\n",
      "test Set: Epoch 11. loss: 2.713399\n",
      "Training\n",
      "train Set: Epoch 12. loss: 2.232249\n",
      "Testing\n",
      "test Set: Epoch 12. loss: 4.392668\n",
      "Training\n",
      "train Set: Epoch 13. loss: 1.976309\n",
      "Testing\n",
      "test Set: Epoch 13. loss: 5.672332\n",
      "Training\n",
      "train Set: Epoch 14. loss: 2.066170\n",
      "Testing\n",
      "test Set: Epoch 14. loss: 4.070884\n",
      "Training\n",
      "train Set: Epoch 15. loss: 1.731062\n",
      "Testing\n",
      "test Set: Epoch 15. loss: 3.226497\n",
      "Training\n",
      "train Set: Epoch 16. loss: 1.512066\n",
      "Testing\n",
      "test Set: Epoch 16. loss: 3.213665\n",
      "Training\n",
      "train Set: Epoch 17. loss: 1.491388\n",
      "Testing\n",
      "test Set: Epoch 17. loss: 3.203370\n",
      "Training\n",
      "train Set: Epoch 18. loss: 1.477755\n",
      "Testing\n",
      "test Set: Epoch 18. loss: 3.195834\n",
      "Training\n",
      "train Set: Epoch 19. loss: 1.464705\n",
      "Testing\n",
      "test Set: Epoch 19. loss: 3.223310\n",
      "Training\n",
      "train Set: Epoch 20. loss: 1.467036\n",
      "Testing\n",
      "test Set: Epoch 20. loss: 3.177773\n",
      "Training\n",
      "train Set: Epoch 21. loss: 1.434153\n",
      "Testing\n",
      "test Set: Epoch 21. loss: 3.181572\n",
      "Training\n",
      "train Set: Epoch 22. loss: 1.406741\n",
      "Testing\n",
      "test Set: Epoch 22. loss: 3.169515\n",
      "Training\n",
      "train Set: Epoch 23. loss: 1.372074\n",
      "Testing\n",
      "test Set: Epoch 23. loss: 3.158133\n",
      "Training\n",
      "train Set: Epoch 24. loss: 1.330743\n",
      "Testing\n",
      "test Set: Epoch 24. loss: 3.145119\n",
      "Training\n",
      "train Set: Epoch 25. loss: 1.279591\n",
      "Testing\n",
      "test Set: Epoch 25. loss: 3.127715\n",
      "Training\n",
      "train Set: Epoch 26. loss: 1.225725\n",
      "Testing\n",
      "test Set: Epoch 26. loss: 3.153253\n",
      "Training\n",
      "train Set: Epoch 27. loss: 1.176124\n",
      "Testing\n",
      "test Set: Epoch 27. loss: 2.884660\n",
      "Training\n",
      "train Set: Epoch 28. loss: 1.124898\n",
      "Testing\n",
      "test Set: Epoch 28. loss: 2.687622\n",
      "Training\n",
      "train Set: Epoch 29. loss: 1.237432\n",
      "Testing\n",
      "test Set: Epoch 29. loss: 3.193635\n",
      "Training\n",
      "train Set: Epoch 30. loss: 0.992265\n",
      "Testing\n",
      "test Set: Epoch 30. loss: 2.785941\n",
      "Training\n",
      "train Set: Epoch 01. loss: 5.124760\n",
      "Testing\n",
      "test Set: Epoch 01. loss: 3.605534\n",
      "Training\n",
      "train Set: Epoch 02. loss: 2.773898\n",
      "Testing\n",
      "test Set: Epoch 02. loss: 3.329974\n",
      "Training\n",
      "train Set: Epoch 03. loss: 2.284917\n",
      "Testing\n",
      "test Set: Epoch 03. loss: 3.468803\n",
      "Training\n",
      "train Set: Epoch 04. loss: 2.295451\n",
      "Testing\n",
      "test Set: Epoch 04. loss: 3.386101\n",
      "Training\n",
      "train Set: Epoch 05. loss: 2.273824\n",
      "Testing\n",
      "test Set: Epoch 05. loss: 3.413977\n",
      "Training\n",
      "train Set: Epoch 06. loss: 2.274688\n",
      "Testing\n",
      "test Set: Epoch 06. loss: 3.403187\n",
      "Training\n",
      "train Set: Epoch 07. loss: 2.267591\n",
      "Testing\n",
      "test Set: Epoch 07. loss: 3.401785\n",
      "Training\n",
      "train Set: Epoch 08. loss: 2.262606\n",
      "Testing\n",
      "test Set: Epoch 08. loss: 3.400510\n",
      "Training\n",
      "train Set: Epoch 09. loss: 2.257654\n",
      "Testing\n",
      "test Set: Epoch 09. loss: 3.398019\n",
      "Training\n",
      "train Set: Epoch 10. loss: 2.252643\n",
      "Testing\n",
      "test Set: Epoch 10. loss: 3.396308\n",
      "Training\n",
      "train Set: Epoch 11. loss: 2.247953\n",
      "Testing\n",
      "test Set: Epoch 11. loss: 3.394483\n",
      "Training\n",
      "train Set: Epoch 12. loss: 2.243444\n",
      "Testing\n",
      "test Set: Epoch 12. loss: 3.392720\n",
      "Training\n",
      "train Set: Epoch 13. loss: 2.239154\n",
      "Testing\n",
      "test Set: Epoch 13. loss: 3.391028\n",
      "Training\n",
      "train Set: Epoch 14. loss: 2.243318\n",
      "Testing\n",
      "test Set: Epoch 14. loss: 3.416049\n",
      "Training\n",
      "train Set: Epoch 15. loss: 2.244460\n",
      "Testing\n",
      "test Set: Epoch 15. loss: 3.437526\n",
      "Training\n",
      "train Set: Epoch 16. loss: 2.273679\n",
      "Testing\n",
      "test Set: Epoch 16. loss: 3.375371\n",
      "Training\n",
      "train Set: Epoch 17. loss: 2.248326\n",
      "Testing\n",
      "test Set: Epoch 17. loss: 3.396216\n",
      "Training\n",
      "train Set: Epoch 18. loss: 2.243050\n",
      "Testing\n",
      "test Set: Epoch 18. loss: 3.390302\n",
      "Training\n",
      "train Set: Epoch 19. loss: 2.235336\n",
      "Testing\n",
      "test Set: Epoch 19. loss: 3.386166\n",
      "Training\n",
      "train Set: Epoch 20. loss: 2.229057\n",
      "Testing\n",
      "test Set: Epoch 20. loss: 3.384661\n",
      "Training\n",
      "train Set: Epoch 21. loss: 2.223822\n",
      "Testing\n",
      "test Set: Epoch 21. loss: 3.382000\n",
      "Training\n",
      "train Set: Epoch 22. loss: 2.218851\n",
      "Testing\n",
      "test Set: Epoch 22. loss: 3.379493\n",
      "Training\n",
      "train Set: Epoch 23. loss: 2.214468\n",
      "Testing\n",
      "test Set: Epoch 23. loss: 3.374067\n",
      "Training\n",
      "train Set: Epoch 24. loss: 2.211564\n",
      "Testing\n",
      "test Set: Epoch 24. loss: 3.379304\n",
      "Training\n",
      "train Set: Epoch 25. loss: 2.234274\n",
      "Testing\n",
      "test Set: Epoch 25. loss: 3.388128\n",
      "Training\n",
      "train Set: Epoch 26. loss: 2.280747\n",
      "Testing\n",
      "test Set: Epoch 26. loss: 3.432581\n",
      "Training\n",
      "train Set: Epoch 27. loss: 2.304407\n",
      "Testing\n",
      "test Set: Epoch 27. loss: 3.422722\n",
      "Training\n",
      "train Set: Epoch 28. loss: 2.300541\n",
      "Testing\n",
      "test Set: Epoch 28. loss: 3.349913\n",
      "Training\n",
      "train Set: Epoch 29. loss: 2.286187\n",
      "Testing\n",
      "test Set: Epoch 29. loss: 3.381502\n",
      "Training\n",
      "train Set: Epoch 30. loss: 2.678680\n",
      "Testing\n",
      "test Set: Epoch 30. loss: 3.443491\n",
      "Training\n",
      "train Set: Epoch 01. loss: 1.347372\n",
      "Testing\n",
      "test Set: Epoch 01. loss: 0.327532\n",
      "Training\n",
      "train Set: Epoch 02. loss: 0.304637\n",
      "Testing\n",
      "test Set: Epoch 02. loss: 0.370239\n",
      "Training\n",
      "train Set: Epoch 03. loss: 0.371929\n",
      "Testing\n",
      "test Set: Epoch 03. loss: 0.457780\n",
      "Training\n",
      "train Set: Epoch 04. loss: 0.348850\n",
      "Testing\n",
      "test Set: Epoch 04. loss: 0.398553\n",
      "Training\n",
      "train Set: Epoch 05. loss: 0.347261\n",
      "Testing\n",
      "test Set: Epoch 05. loss: 0.333990\n",
      "Training\n",
      "train Set: Epoch 06. loss: 0.328387\n",
      "Testing\n",
      "test Set: Epoch 06. loss: 0.331076\n",
      "Training\n",
      "train Set: Epoch 07. loss: 0.319923\n",
      "Testing\n",
      "test Set: Epoch 07. loss: 0.335412\n",
      "Training\n",
      "train Set: Epoch 08. loss: 0.318863\n",
      "Testing\n",
      "test Set: Epoch 08. loss: 0.331998\n",
      "Training\n",
      "train Set: Epoch 09. loss: 0.314619\n",
      "Testing\n",
      "test Set: Epoch 09. loss: 0.328114\n",
      "Training\n",
      "train Set: Epoch 10. loss: 0.309039\n",
      "Testing\n",
      "test Set: Epoch 10. loss: 0.325984\n",
      "Training\n",
      "train Set: Epoch 11. loss: 0.303947\n",
      "Testing\n",
      "test Set: Epoch 11. loss: 0.324766\n",
      "Training\n",
      "train Set: Epoch 12. loss: 0.299545\n",
      "Testing\n",
      "test Set: Epoch 12. loss: 0.323788\n",
      "Training\n",
      "train Set: Epoch 13. loss: 0.295498\n",
      "Testing\n",
      "test Set: Epoch 13. loss: 0.323002\n",
      "Training\n",
      "train Set: Epoch 14. loss: 0.291605\n",
      "Testing\n",
      "test Set: Epoch 14. loss: 0.322497\n",
      "Training\n",
      "train Set: Epoch 15. loss: 0.287777\n",
      "Testing\n",
      "test Set: Epoch 15. loss: 0.322329\n",
      "Training\n",
      "train Set: Epoch 16. loss: 0.283918\n",
      "Testing\n",
      "test Set: Epoch 16. loss: 0.322581\n",
      "Training\n",
      "train Set: Epoch 17. loss: 0.279880\n",
      "Testing\n",
      "test Set: Epoch 17. loss: 0.592209\n",
      "Training\n",
      "train Set: Epoch 18. loss: 0.319114\n",
      "Testing\n",
      "test Set: Epoch 18. loss: 0.432348\n",
      "Training\n",
      "train Set: Epoch 19. loss: 0.294191\n",
      "Testing\n",
      "test Set: Epoch 19. loss: 0.303573\n",
      "Training\n",
      "train Set: Epoch 20. loss: 0.267621\n",
      "Testing\n",
      "test Set: Epoch 20. loss: 0.317014\n",
      "Training\n",
      "train Set: Epoch 21. loss: 0.254147\n",
      "Testing\n",
      "test Set: Epoch 21. loss: 0.334605\n",
      "Training\n",
      "train Set: Epoch 22. loss: 0.250323\n",
      "Testing\n",
      "test Set: Epoch 22. loss: 0.336722\n",
      "Training\n",
      "train Set: Epoch 23. loss: 0.243704\n",
      "Testing\n",
      "test Set: Epoch 23. loss: 0.345855\n",
      "Training\n",
      "train Set: Epoch 24. loss: 0.247535\n",
      "Testing\n",
      "test Set: Epoch 24. loss: 0.363290\n",
      "Training\n",
      "train Set: Epoch 25. loss: 0.232896\n",
      "Testing\n",
      "test Set: Epoch 25. loss: 0.360657\n",
      "Training\n",
      "train Set: Epoch 26. loss: 0.225332\n",
      "Testing\n",
      "test Set: Epoch 26. loss: 0.367123\n",
      "Training\n",
      "train Set: Epoch 27. loss: 0.202661\n",
      "Testing\n",
      "test Set: Epoch 27. loss: 0.422224\n",
      "Training\n",
      "train Set: Epoch 28. loss: 0.278836\n",
      "Testing\n",
      "test Set: Epoch 28. loss: 0.293312\n",
      "Training\n",
      "train Set: Epoch 29. loss: 0.303953\n",
      "Testing\n",
      "test Set: Epoch 29. loss: 0.513622\n",
      "Training\n",
      "train Set: Epoch 30. loss: 0.242149\n",
      "Testing\n",
      "test Set: Epoch 30. loss: 0.659092\n",
      "Training\n",
      "train Set: Epoch 01. loss: 3.044062\n",
      "Testing\n",
      "test Set: Epoch 01. loss: 2.896209\n",
      "Training\n",
      "train Set: Epoch 02. loss: 1.889914\n",
      "Testing\n",
      "test Set: Epoch 02. loss: 3.944217\n",
      "Training\n",
      "train Set: Epoch 03. loss: 2.095002\n",
      "Testing\n",
      "test Set: Epoch 03. loss: 4.348447\n",
      "Training\n",
      "train Set: Epoch 04. loss: 2.035185\n",
      "Testing\n",
      "test Set: Epoch 04. loss: 3.086047\n",
      "Training\n",
      "train Set: Epoch 05. loss: 1.817157\n",
      "Testing\n",
      "test Set: Epoch 05. loss: 3.383000\n",
      "Training\n",
      "train Set: Epoch 06. loss: 1.966542\n",
      "Testing\n",
      "test Set: Epoch 06. loss: 3.981282\n",
      "Training\n",
      "train Set: Epoch 07. loss: 1.814555\n",
      "Testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Set: Epoch 07. loss: 2.963758\n",
      "Training\n",
      "train Set: Epoch 08. loss: 1.514698\n",
      "Testing\n",
      "test Set: Epoch 08. loss: 3.031283\n",
      "Training\n",
      "train Set: Epoch 09. loss: 1.502712\n",
      "Testing\n",
      "test Set: Epoch 09. loss: 3.026674\n",
      "Training\n",
      "train Set: Epoch 10. loss: 1.487416\n",
      "Testing\n",
      "test Set: Epoch 10. loss: 3.016146\n",
      "Training\n",
      "train Set: Epoch 11. loss: 1.511462\n",
      "Testing\n",
      "test Set: Epoch 11. loss: 3.049917\n",
      "Training\n",
      "train Set: Epoch 12. loss: 1.513200\n",
      "Testing\n",
      "test Set: Epoch 12. loss: 2.995603\n",
      "Training\n",
      "train Set: Epoch 13. loss: 1.513807\n",
      "Testing\n",
      "test Set: Epoch 13. loss: 3.018176\n",
      "Training\n",
      "train Set: Epoch 14. loss: 1.481147\n",
      "Testing\n",
      "test Set: Epoch 14. loss: 3.006439\n",
      "Training\n",
      "train Set: Epoch 15. loss: 1.470636\n",
      "Testing\n",
      "test Set: Epoch 15. loss: 3.010272\n",
      "Training\n",
      "train Set: Epoch 16. loss: 1.461360\n",
      "Testing\n",
      "test Set: Epoch 16. loss: 2.947074\n",
      "Training\n",
      "train Set: Epoch 17. loss: 1.480300\n",
      "Testing\n",
      "test Set: Epoch 17. loss: 3.002469\n",
      "Training\n",
      "train Set: Epoch 18. loss: 1.450784\n",
      "Testing\n",
      "test Set: Epoch 18. loss: 2.989705\n",
      "Training\n",
      "train Set: Epoch 19. loss: 1.424950\n",
      "Testing\n",
      "test Set: Epoch 19. loss: 3.014409\n",
      "Training\n",
      "train Set: Epoch 20. loss: 1.465826\n",
      "Testing\n",
      "test Set: Epoch 20. loss: 3.200144\n",
      "Training\n",
      "train Set: Epoch 21. loss: 1.479403\n",
      "Testing\n",
      "test Set: Epoch 21. loss: 2.912506\n",
      "Training\n",
      "train Set: Epoch 22. loss: 1.438124\n",
      "Testing\n",
      "test Set: Epoch 22. loss: 3.227786\n",
      "Training\n",
      "train Set: Epoch 23. loss: 1.495953\n",
      "Testing\n",
      "test Set: Epoch 23. loss: 2.901924\n",
      "Training\n",
      "train Set: Epoch 24. loss: 1.362779\n",
      "Testing\n",
      "test Set: Epoch 24. loss: 3.390723\n",
      "Training\n",
      "train Set: Epoch 25. loss: 1.385967\n",
      "Testing\n",
      "test Set: Epoch 25. loss: 3.415380\n",
      "Training\n",
      "train Set: Epoch 26. loss: 1.306774\n",
      "Testing\n",
      "test Set: Epoch 26. loss: 2.735417\n",
      "Training\n",
      "train Set: Epoch 27. loss: 1.126320\n",
      "Testing\n",
      "test Set: Epoch 27. loss: 2.577669\n",
      "Training\n",
      "train Set: Epoch 28. loss: 1.013385\n",
      "Testing\n",
      "test Set: Epoch 28. loss: 2.508159\n",
      "Training\n",
      "train Set: Epoch 29. loss: 0.921189\n",
      "Testing\n",
      "test Set: Epoch 29. loss: 2.475935\n",
      "Training\n",
      "train Set: Epoch 30. loss: 0.815437\n",
      "Testing\n",
      "test Set: Epoch 30. loss: 2.390860\n",
      "Training\n",
      "train Set: Epoch 01. loss: 4.821634\n",
      "Testing\n",
      "test Set: Epoch 01. loss: 3.631695\n",
      "Training\n",
      "train Set: Epoch 02. loss: 2.633816\n",
      "Testing\n",
      "test Set: Epoch 02. loss: 2.539126\n",
      "Training\n",
      "train Set: Epoch 03. loss: 2.291405\n",
      "Testing\n",
      "test Set: Epoch 03. loss: 2.537758\n",
      "Training\n",
      "train Set: Epoch 04. loss: 2.209904\n",
      "Testing\n",
      "test Set: Epoch 04. loss: 2.545261\n",
      "Training\n",
      "train Set: Epoch 05. loss: 2.238650\n",
      "Testing\n",
      "test Set: Epoch 05. loss: 2.536666\n",
      "Training\n",
      "train Set: Epoch 06. loss: 2.220688\n",
      "Testing\n",
      "test Set: Epoch 06. loss: 2.537455\n",
      "Training\n",
      "train Set: Epoch 07. loss: 2.220431\n",
      "Testing\n",
      "test Set: Epoch 07. loss: 2.539577\n",
      "Training\n",
      "train Set: Epoch 08. loss: 2.234512\n",
      "Testing\n",
      "test Set: Epoch 08. loss: 2.530540\n",
      "Training\n",
      "train Set: Epoch 09. loss: 2.242749\n",
      "Testing\n",
      "test Set: Epoch 09. loss: 2.537918\n",
      "Training\n",
      "train Set: Epoch 10. loss: 2.226822\n",
      "Testing\n",
      "test Set: Epoch 10. loss: 2.538581\n",
      "Training\n",
      "train Set: Epoch 11. loss: 2.226793\n",
      "Testing\n",
      "test Set: Epoch 11. loss: 2.574818\n",
      "Training\n",
      "train Set: Epoch 12. loss: 2.262439\n",
      "Testing\n",
      "test Set: Epoch 12. loss: 2.540665\n",
      "Training\n",
      "train Set: Epoch 13. loss: 2.195106\n",
      "Testing\n",
      "test Set: Epoch 13. loss: 2.541235\n",
      "Training\n",
      "train Set: Epoch 14. loss: 2.192344\n",
      "Testing\n",
      "test Set: Epoch 14. loss: 2.542291\n",
      "Training\n",
      "train Set: Epoch 15. loss: 2.191840\n",
      "Testing\n",
      "test Set: Epoch 15. loss: 2.542540\n",
      "Training\n",
      "train Set: Epoch 16. loss: 2.190997\n",
      "Testing\n",
      "test Set: Epoch 16. loss: 2.542422\n",
      "Training\n",
      "train Set: Epoch 17. loss: 2.190015\n",
      "Testing\n",
      "test Set: Epoch 17. loss: 2.542277\n",
      "Training\n",
      "train Set: Epoch 18. loss: 2.189113\n",
      "Testing\n",
      "test Set: Epoch 18. loss: 2.542167\n",
      "Training\n",
      "train Set: Epoch 19. loss: 2.188323\n",
      "Testing\n",
      "test Set: Epoch 19. loss: 2.542082\n",
      "Training\n",
      "train Set: Epoch 20. loss: 2.187621\n",
      "Testing\n",
      "test Set: Epoch 20. loss: 2.542009\n",
      "Training\n",
      "train Set: Epoch 21. loss: 2.186984\n",
      "Testing\n",
      "test Set: Epoch 21. loss: 2.541940\n",
      "Training\n",
      "train Set: Epoch 22. loss: 2.186394\n",
      "Testing\n",
      "test Set: Epoch 22. loss: 2.541873\n",
      "Training\n",
      "train Set: Epoch 23. loss: 2.185843\n",
      "Testing\n",
      "test Set: Epoch 23. loss: 2.541809\n",
      "Training\n",
      "train Set: Epoch 24. loss: 2.185325\n",
      "Testing\n",
      "test Set: Epoch 24. loss: 2.541746\n",
      "Training\n",
      "train Set: Epoch 25. loss: 2.184835\n",
      "Testing\n",
      "test Set: Epoch 25. loss: 2.541684\n",
      "Training\n",
      "train Set: Epoch 26. loss: 2.184369\n",
      "Testing\n",
      "test Set: Epoch 26. loss: 2.541622\n",
      "Training\n",
      "train Set: Epoch 27. loss: 2.183926\n",
      "Testing\n",
      "test Set: Epoch 27. loss: 2.541561\n",
      "Training\n",
      "train Set: Epoch 28. loss: 2.183503\n",
      "Testing\n",
      "test Set: Epoch 28. loss: 2.541500\n",
      "Training\n",
      "train Set: Epoch 29. loss: 2.183099\n",
      "Testing\n",
      "test Set: Epoch 29. loss: 2.541437\n",
      "Training\n",
      "train Set: Epoch 30. loss: 2.182712\n",
      "Testing\n",
      "test Set: Epoch 30. loss: 2.541374\n",
      "Training\n",
      "train Set: Epoch 01. loss: 4.220995\n",
      "Testing\n",
      "test Set: Epoch 01. loss: 0.512920\n",
      "Training\n",
      "train Set: Epoch 02. loss: 0.512510\n",
      "Testing\n",
      "test Set: Epoch 02. loss: 0.243410\n",
      "Training\n",
      "train Set: Epoch 03. loss: 0.414721\n",
      "Testing\n",
      "test Set: Epoch 03. loss: 0.272275\n",
      "Training\n",
      "train Set: Epoch 04. loss: 0.421682\n",
      "Testing\n",
      "test Set: Epoch 04. loss: 0.217152\n",
      "Training\n",
      "train Set: Epoch 05. loss: 0.368994\n",
      "Testing\n",
      "test Set: Epoch 05. loss: 0.224651\n",
      "Training\n",
      "train Set: Epoch 06. loss: 0.375170\n",
      "Testing\n",
      "test Set: Epoch 06. loss: 0.219212\n",
      "Training\n",
      "train Set: Epoch 07. loss: 0.371530\n",
      "Testing\n",
      "test Set: Epoch 07. loss: 0.218692\n",
      "Training\n",
      "train Set: Epoch 08. loss: 0.367702\n",
      "Testing\n",
      "test Set: Epoch 08. loss: 0.215368\n",
      "Training\n",
      "train Set: Epoch 09. loss: 0.362724\n",
      "Testing\n",
      "test Set: Epoch 09. loss: 0.212727\n",
      "Training\n",
      "train Set: Epoch 10. loss: 0.357067\n",
      "Testing\n",
      "test Set: Epoch 10. loss: 0.210014\n",
      "Training\n",
      "train Set: Epoch 11. loss: 0.351293\n",
      "Testing\n",
      "test Set: Epoch 11. loss: 0.207755\n",
      "Training\n",
      "train Set: Epoch 12. loss: 0.345451\n",
      "Testing\n",
      "test Set: Epoch 12. loss: 0.205993\n",
      "Training\n",
      "train Set: Epoch 13. loss: 0.340211\n",
      "Testing\n",
      "test Set: Epoch 13. loss: 0.204836\n",
      "Training\n",
      "train Set: Epoch 14. loss: 0.331951\n",
      "Testing\n",
      "test Set: Epoch 14. loss: 0.266270\n",
      "Training\n",
      "train Set: Epoch 15. loss: 0.322169\n",
      "Testing\n",
      "test Set: Epoch 15. loss: 0.307601\n",
      "Training\n",
      "train Set: Epoch 16. loss: 0.351557\n",
      "Testing\n",
      "test Set: Epoch 16. loss: 0.381886\n",
      "Training\n",
      "train Set: Epoch 17. loss: 0.362259\n",
      "Testing\n",
      "test Set: Epoch 17. loss: 0.208784\n",
      "Training\n",
      "train Set: Epoch 18. loss: 0.320963\n",
      "Testing\n",
      "test Set: Epoch 18. loss: 0.355201\n",
      "Training\n",
      "train Set: Epoch 19. loss: 0.377852\n",
      "Testing\n",
      "test Set: Epoch 19. loss: 0.435239\n",
      "Training\n",
      "train Set: Epoch 20. loss: 0.378102\n",
      "Testing\n",
      "test Set: Epoch 20. loss: 0.484603\n",
      "Training\n",
      "train Set: Epoch 21. loss: 0.381153\n",
      "Testing\n",
      "test Set: Epoch 21. loss: 0.549318\n",
      "Training\n",
      "train Set: Epoch 22. loss: 0.404651\n",
      "Testing\n",
      "test Set: Epoch 22. loss: 0.462915\n",
      "Training\n",
      "train Set: Epoch 23. loss: 0.338230\n",
      "Testing\n",
      "test Set: Epoch 23. loss: 0.374797\n",
      "Training\n",
      "train Set: Epoch 24. loss: 0.371136\n",
      "Testing\n",
      "test Set: Epoch 24. loss: 0.372609\n",
      "Training\n",
      "train Set: Epoch 25. loss: 0.343822\n",
      "Testing\n",
      "test Set: Epoch 25. loss: 0.216281\n",
      "Training\n",
      "train Set: Epoch 26. loss: 0.293448\n",
      "Testing\n",
      "test Set: Epoch 26. loss: 0.217903\n",
      "Training\n",
      "train Set: Epoch 27. loss: 0.277223\n",
      "Testing\n",
      "test Set: Epoch 27. loss: 0.223209\n",
      "Training\n",
      "train Set: Epoch 28. loss: 0.282063\n",
      "Testing\n",
      "test Set: Epoch 28. loss: 0.219619\n",
      "Training\n",
      "train Set: Epoch 29. loss: 0.278277\n",
      "Testing\n",
      "test Set: Epoch 29. loss: 0.220443\n",
      "Training\n",
      "train Set: Epoch 30. loss: 0.277205\n",
      "Testing\n",
      "test Set: Epoch 30. loss: 0.220148\n"
     ]
    }
   ],
   "source": [
    "## Current nc and anomaly files at 105 m depth \n",
    "depth_arr = np.array([105, 155, 205])\n",
    "month_lead_arr = np.array([2, 6, 12])\n",
    "\n",
    "for mm in range(len(month_lead_arr)):\n",
    "  for ii in range(len(depth_arr)):\n",
    "    depth_sel = depth_arr[ii]\n",
    "    month_lead = month_lead_arr[mm]\n",
    "    lead_time = month_lead \n",
    "    \n",
    "    depth_string = str(depth_sel)\n",
    "    godas_path = '/cw3e/mead/projects/cdd101/Files/GODAS/godasData_'+depth_string+'m.nc'\n",
    "    anomaly_path = '/cw3e/mead/projects/cdd101/Files/GODAS/fixed_deepTemp_anomalies_'+depth_string+'m.txt'\n",
    "\n",
    "    #Assemble numpy arrays corresponding to predictors and predictands\n",
    "    train_start_date = '1980-01-01'\n",
    "    #train_end_date = '2003-12-31'\n",
    "    train_end_date = '1995-12-31'\n",
    "\n",
    "    # average between 3 months \n",
    "    num_input_time_steps = 3\n",
    "\n",
    "    train_predictors, train_predictands = assemble_predictors_predictands(train_start_date,\n",
    "                          train_end_date, lead_time, 'observations', 'spatial', num_input_time_steps=num_input_time_steps)\n",
    "    test_predictors, test_predictands = assemble_predictors_predictands('1997-01-01',\n",
    "                        '2002-12-31', lead_time, 'observations', 'spatial', num_input_time_steps=num_input_time_steps)\n",
    "\n",
    "    #Convert the numpy ararys into ENSODataset, which is a subset of the \n",
    "    #torch.utils.data.Dataset class.  This class is compatible with\n",
    "    #the torch dataloader, which allows for data loading for a CNN\n",
    "    train_dataset = ENSODataset(train_predictors, train_predictands)\n",
    "    test_dataset = ENSODataset(test_predictors, test_predictands)\n",
    "\n",
    "    #Create a torch.utils.data.DataLoader from the ENSODatasets() created earlier!\n",
    "    #the similarity between the name DataLoader and Dataset in the pytorch API is unfortunate...\n",
    "    trainloader = DataLoader(train_dataset, batch_size=10)\n",
    "    testloader = DataLoader(test_dataset, batch_size=10)\n",
    "\n",
    "    # change lr \n",
    "    net = CNN(num_input_time_steps=num_input_time_steps)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "    # review nn.MSELoss, it has issues with X and Y \n",
    "    experiment_name = \"twolayerCNN_{}_{}\".format(train_start_date, train_end_date)\n",
    "    predictions, train_losses, test_losses = train_network(net, nn.MSELoss(), \n",
    "                      optimizer, trainloader, testloader, experiment_name)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Performance of {} Neural Network During Training'.format(experiment_name))\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    images_dir = '/cw3e/mead/projects/cdd101/Figures/NOAA_GODAS_Results_NeuralNetwork'\n",
    "    figName_png = str(depth_sel)+'m_Loss_'+str(month_lead)+'month.png'\n",
    "    figName_pdf = str(depth_sel)+'m_Loss_'+str(month_lead)+'month.pdf'\n",
    "    plt.savefig(images_dir+\"/\"+figName_png, bbox_inches='tight')\n",
    "    plt.savefig(images_dir+\"/\"+figName_pdf, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    corr, _ = pearsonr(test_predictands, predictions)\n",
    "    rmse = mean_squared_error(test_predictands, predictions) ** 0.5\n",
    "    plot_nino_time_series(test_predictands, predictions, depth_sel, month_lead, '{} Predictions. Corr: {:3f}. RMSE: {:3f}.'.format(experiment_name,\n",
    "                                                                          corr, rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
